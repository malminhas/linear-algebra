{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5870b12-5127-4e85-9459-50d2b3b76a8f",
   "metadata": {},
   "source": [
    "# Linear Algebra Playbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce4ebbb-da37-44f4-a905-24a33f2575d4",
   "metadata": {},
   "source": [
    "<p>\n",
    "Mal Minhas, v0.1<br>\n",
    "29.12.22\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e50ef-2d57-4bff-b705-146eb6f1c8d7",
   "metadata": {},
   "source": [
    "## Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46da42-bb2a-4334-895d-734a20219933",
   "metadata": {},
   "source": [
    "Numerous problems in engineering and science can be described or approximated by linear relationships.  The study of linear relationship is contained in the field of **linear algebra**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26b369f-69da-445c-bc1f-e3ddbaf27d4f",
   "metadata": {},
   "source": [
    "## Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33575f9e-fae7-45d1-9656-0468684c22bd",
   "metadata": {},
   "source": [
    "The set ${\\mathbb{R}}^n$ is the set of all ğ‘›-tuples of real numbers. In set notation this is ${\\mathbb{R}}^n = \\{(x_1,x_2,x_3,...,x_n):x_1,x_2,x_3,...,x_n{\\in}{\\mathbb{R}}^n\\}$. For example, the set ${\\mathbb{R}}^3$ represents the set of real triples, $({x},{y},{z})$ coordinates, in three-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df3ea9-0792-4db4-8960-7cf17003e359",
   "metadata": {},
   "source": [
    "A vector in ${\\mathbb{R}}^n$ is an ${n}$-tuple, or point, in ${\\mathbb{R}}^n$. Vectors can be written horizontally (i.e., with the elements of the vector next to each other) in a **row vector**, or vertically (i.e., with the elements of the vector on top of each other) in a **column vector**. If the context of a vector is ambiguous, it usually means the vector is a column vector. The ${i}$-th element of a vector, ${v}$, is denoted by  ${v}_i$. The transpose of a column vector is a row vector of the same length, and the transpose of a row vector is a column vector. In mathematics, the transpose is denoted by a superscript ${T}$, or  ${v}^T$. The zero vector is the vector in ${\\mathbb{R}}^n$ containing all zeros.  The following Python code creates a row vector and a separate column vector, and shows the shape of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "203d8ff6-db87-4a75-b090-91476e5d29ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 -5  3  2  4]] of shape (1, 5)\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]] of shape (4, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "row_vector = np.array([[1, -5, 3, 2, 4]])\n",
    "column_vector = np.array([[1], \n",
    "                          [2], \n",
    "                          [3], \n",
    "                          [4]])\n",
    "print(f'{row_vector} of shape {row_vector.shape}')\n",
    "print(f'{column_vector} of shape {column_vector.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e73bb8c-3e69-4a24-a145-0d42021e38e7",
   "metadata": {},
   "source": [
    "The **norm** of a vector is a measure of its length. There are many ways of defining the length of a vector depending on the metric used (i.e., the distance formula chosen). The most common is called the **$L_2$ norm**, which is computed according to the distance formula you are probably familiar with from school. The $L_2$ norm of a vector $v$ is denoted by $||{v}||_2$ and $||{v}|| = \\sqrt{{\\sum_i}{v_i^2}}$. This is sometimes also called Euclidian length and refers to the â€œphysicalâ€ length of a vector in 1-D, 2-D, or 3-D space. The **$L_1$ norm**, or â€œManhattan Distance,â€ is computed as $||{v}||_1 = {\\sum_i}|{v_i}|$, and is named after the grid-like road structure in New York City. The **p-norm**, $L_p$, of a vector is $\\vert\\vert{v}\\vert\\vert_p = \\sqrt[p]{{\\sum_i}{v_i^p}}$. The **$L_\\infty$ norm**, where $p=\\infty$ is written as $||ğ‘£||_\\infty$.  The $L_\\infty$ norm is equal to the maximum absolute value in $v$.\n",
    "\n",
    "Here we transpose the row vector defined above into a column vector and calculate its $L_1$, $L_2$, and $L_\\infty$ norms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4a42779-0450-4875-911e-5176ff9d649b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1]\n",
      " [-5]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 4]]\n",
      "L_1 is: 15.0\n",
      "L_2 is: 7.4\n",
      "L_inf is: 5.0\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "new_vector = row_vector.T\n",
    "print(new_vector)\n",
    "norm_1 = norm(new_vector, 1)\n",
    "norm_2 = norm(new_vector, 2)\n",
    "norm_inf = norm(new_vector, np.inf)\n",
    "print('L_1 is: %.1f'%norm_1)\n",
    "print('L_2 is: %.1f'%norm_2)\n",
    "print('L_inf is: %.1f'%norm_inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b3dcf-4887-49ec-aea7-a6784a1e3dac",
   "metadata": {},
   "source": [
    "**Vector addition** is defined as the pairwise addition of each of the elements of the added vectors. For example, if $v$ and $w$ are vectors in ${\\mathbb{R}}^n$, then $u=v+w$ is defined as $u_i=v_i+w_i$.\n",
    "\n",
    "**Vector multiplication** can be defined in several ways depending on the context. **Scalar multiplication** of a vector is the product of a vector and a scalar (i.e., a number in ${\\mathbb{R}}^n$). Scalar multiplication is defined as the product of each element of the vector by the scalar. More specifically, if $\\alpha$ is a scalar and $v$ is a vector, then \n",
    "$u = \\alpha{v}$ is defined as $u_i = \\alpha{v_i}$. Note that this is exactly how Python implements scalar multiplication with a vector.\n",
    "\n",
    "The **dot product** of two vectors is the sum of the product of the respective elements in each vector and is denoted by $.$, and $v.{w}$ is read â€œv dot w.â€ Therefore for $v$ and $w$ $\\in{\\mathbb{R}}^n, d=v.w$ is defined as $d = \\sum\\limits_{i=1}^n {v_i}{w_i}$. The angle between two vectors, $\\theta$, is defined by the formula:\n",
    "\n",
    "$v.w = ||v||_2||w||_2cos\\theta$\n",
    "\n",
    "The dot product is a measure of how similarly directed the two vectors are.  In the following code we are calculating the angle between two vectors using dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "254c6d46-2bd4-40fa-96b5-65940f53ab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.97992471]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import arccos, dot\n",
    "\n",
    "v = np.array([[10, 9, 3]])\n",
    "w = np.array([[2, 5, 12]])\n",
    "theta = arccos(dot(v, w.T)/(norm(v)*norm(w)))\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c9d4e-a481-4d33-96e1-d74dbf294432",
   "metadata": {},
   "source": [
    "Finally, the **cross product** between two vectors, $v$ and $w$, is written ${v}\\times{w}$. It is defined by ${v}\\times{w} = ||v||_2||w||_2\\sin(\\theta){n}$, where $\\theta$ is the angle between the $v$ and $w$ (which can be computed from the dot product) and $n$ is a vector perpendicular to both $v$ and $w$ with unit length (i.e., the length is one). The geometric interpretation of the cross product is a vector perpendicular to both $v$ and $w$ with length equal to the area enclosed by the parallelogram created by the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06ac3295-dee9-4311-8950-049c2af419d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0 -6]]\n"
     ]
    }
   ],
   "source": [
    "v = np.array([[0, 2, 0]])\n",
    "w = np.array([[3, 0, 0]])\n",
    "print(np.cross(v, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa39e8-90f5-4b5e-a433-c37ca7228419",
   "metadata": {},
   "source": [
    "A set of vectors is said to be **linearly independent** if no vector in the set can be written as a linear combination of the other vectors in the set.  A set of vectors that is not linearly independent is linearly dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d866b2-2d25-4768-be41-77631951091e",
   "metadata": {},
   "source": [
    "## Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33897028-6bfb-4509-8ab8-220340db8168",
   "metadata": {},
   "source": [
    "An ${m}\\times{n}$ matrix is a rectangular table of numbers consisting of $m$ rows and $n$ columns. The norm of a matrix can be considered as a particular kind of vector norm, if we treat the ${m}\\times{n}$ elements of $M$ are the elements of an ${m}{n}$ dimensional vector, then you can calculate the matrix norm using the same norm function in `numpy` as that for vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4575d191-92af-4107-9d14-7d3219786755",
   "metadata": {},
   "source": [
    "**Matrix multiplication** between two matrices, $P$ and $Q$, is defined when $P$ is an ${m}\\times{p}$ matrix and $Q$ is a ${p}\\times{n}$ matrix. The result of $M=PQ$ is a matrix $M$ that is ${m}\\times{n}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c284c-fadd-4301-81d0-9a0a1da36e74",
   "metadata": {},
   "source": [
    "The product of two matrices $P$ and $Q$ in Python is achieved by using the `dot` method in `numpy`. The transpose of a matrix is a reversal of its rows with its columns. The transpose is denoted by a superscript, $T$, such as $MT$ is the transpose of matrix $M$. In Python, the method `T` for an `numpy` `array` is used to get the transpose. For example, if $M$ is a matrix, then $M.T$ is its transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef6e90aa-929d-41c0-97c9-188cf656621b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 7]\n",
      " [2 3]\n",
      " [5 0]]\n",
      "[[2 6 3 1]\n",
      " [1 2 3 4]]\n",
      "[[ 9 20 24 29]\n",
      " [ 7 18 15 14]\n",
      " [10 30 15  5]]\n",
      "[[ 9  7 10]\n",
      " [20 18 30]\n",
      " [24 15 15]\n",
      " [29 14  5]]\n"
     ]
    }
   ],
   "source": [
    "P = np.array([[1, 7], [2, 3], [5, 0]])\n",
    "Q = np.array([[2, 6, 3, 1], [1, 2, 3, 4]])\n",
    "print(P)\n",
    "print(Q)\n",
    "print(np.dot(P, Q))\n",
    "print(np.dot(P, Q).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b51d52-391a-41ce-a416-732884b394d4",
   "metadata": {},
   "source": [
    "A **square matrix** is an ğ‘›Ã—ğ‘› matrix; that is, it has the same number of rows as columns. The **determinant** is an important property of square matrices. The determinant is denoted by ğ‘‘ğ‘’ğ‘¡(ğ‘€), both in mathematics and in Numpyâ€™s linalg package, sometimes it is also denoted as |ğ‘€|. Some examples in the uses of a determinant will be described later.\n",
    "\n",
    "In the case of a 2Ã—2 matrix, the determinant is:\n",
    "\n",
    "|ğ‘€|=[ğ‘ğ‘ğ‘ğ‘‘]=ğ‘ğ‘‘âˆ’ğ‘ğ‘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69440b2-5f37-49a1-a38c-58ffce9c6d8f",
   "metadata": {},
   "source": [
    "The **identity matrix** is a square matrix with ones on the diagonal and zeros elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f1efc47-de43-4930-a386-79f352b61cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:\n",
      " [[0 2 1 3]\n",
      " [3 2 8 1]\n",
      " [1 0 0 3]\n",
      " [0 3 2 1]]\n",
      "Determinant: -38.0\n",
      "I:\n",
      " [[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "M*I:\n",
      " [[0. 2. 1. 3.]\n",
      " [3. 2. 8. 1.]\n",
      " [1. 0. 0. 3.]\n",
      " [0. 3. 2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import det\n",
    "\n",
    "M = np.array([[0,2,1,3], \n",
    "             [3,2,8,1], \n",
    "             [1,0,0,3],\n",
    "             [0,3,2,1]])\n",
    "print('M:\\n', M)\n",
    "\n",
    "print('Determinant: %.1f'%det(M))\n",
    "I = np.eye(4)\n",
    "print('I:\\n', I)\n",
    "print('M*I:\\n', np.dot(M, I))\n",
    "assert((np.dot(M, I) == M).all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f23d95f-98e7-4f5d-9329-98317ab98d07",
   "metadata": {},
   "source": [
    "The **inverse** of a square matrix ğ‘€ is a matrix of the same size, ğ‘, such that ğ‘€â‹…ğ‘=ğ¼. The inverse of a matrix is analagous to the inverse of real numbers. For example, the inverse of 3 is 13 because (3)(13)=1. A matrix is said to be invertible if it has an inverse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d176ff6-8208-445e-a8a1-b6d73ad69b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inv M:\n",
      " [[-1.57894737 -0.07894737  1.23684211  1.10526316]\n",
      " [-0.63157895 -0.13157895  0.39473684  0.84210526]\n",
      " [ 0.68421053  0.18421053 -0.55263158 -0.57894737]\n",
      " [ 0.52631579  0.02631579 -0.07894737 -0.36842105]]\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0. -0.  1. -0.]\n",
      " [ 0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "print('Inv M:\\n', inv(M))\n",
    "print(np.round(np.dot(inv(M), M)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0c9fac-a3c3-47c9-84c7-c6e2073b14d7",
   "metadata": {},
   "source": [
    "For a 2Ã—2 matrix, the analytic solution of the matrix inverse is:\n",
    "\n",
    "ğ‘€âˆ’1=[ğ‘ğ‘ğ‘ğ‘‘]âˆ’1=1|ğ‘€|[ğ‘‘âˆ’ğ‘âˆ’ğ‘ğ‘]\n",
    "\n",
    "The calculation of the matrix inverse for the analytic solution becomes complicated with increasing matrix dimension, there are many other methods can make things easier, such as Gaussian elimination, Newtonâ€™s method, Eigendecomposition and so on. We will introduce some of these methods after we learn how to solve a system of linear equations, because the process is essentially the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db0e8d-fb02-4461-aa03-b598c8bbf29c",
   "metadata": {},
   "source": [
    "Recall that 0 has no inverse for multiplication in the real-numbers setting. Similarly, there are matrices that do not have inverses. These matrices are called **singular**. Matrices that do have an inverse are called **nonsingular**.  One way to determine if a matrix is singular is by computing its determinant. If the determinant is 0, then the matrix is singular; if not, the matrix is nonsingular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1df158ba-16fb-4fcd-b7fe-887a375264e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:\n",
      " [[0 1 0]\n",
      " [0 0 0]\n",
      " [1 0 1]]\n",
      "det(P):\n",
      " 0.0\n"
     ]
    }
   ],
   "source": [
    "P = np.array([[0,1,0],\n",
    "              [0,0,0],\n",
    "              [1,0,1]])\n",
    "print('P:\\n', P)\n",
    "print('det(P):\\n', det(P))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5192c4e6-8efc-4a45-8695-a0eda899fa71",
   "metadata": {},
   "source": [
    "A matrix that is close to being singular (i.e., the determinant is close to 0) is called ill-conditioned. Although ill-conditioned matrices have inverses, they are problematic numerically in the same way that dividing a number by a very, very small number is problematic. The **condition number** is a measure of how ill-conditioned a matrix is, and it can be computed using Numpyâ€™s function cond from linalg. The higher the condition number, the closer the matrix is to being singular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0735265-5656-4594-98b4-3494cb3a1c95",
   "metadata": {},
   "source": [
    "The **rank** of an ğ‘šÃ—ğ‘› matrix ğ´ is the number of linearly independent columns or rows of ğ´, and is denoted by rank(ğ´). It can be shown that the number of linearly independent rows is always equal to the number of linearly independent columns for any matrix. A matrix is called full rank. if rank (ğ´)=min(ğ‘š,ğ‘›). The matrix, ğ´, is also full rank if all of its columns are linearly independent. An augmented matrix. is a matrix, ğ´, concatenated with a vector, ğ‘¦, and is written [ğ´,ğ‘¦]. This is commonly read â€œğ´ augmented with ğ‘¦.â€  You can use np.concatenate to concatenate the them. If ğ‘Ÿğ‘ğ‘›ğ‘˜([ğ´,ğ‘¦])=ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ´)+1, then the vector, ğ‘¦, is â€œnewâ€ information. That is, it cannot be created as a linear combination of the columns in ğ´. The rank is an important property of matrices because of its relationship to solutions of linear equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4a51d0-36b5-40d2-9363-1aa83f0043ba",
   "metadata": {},
   "source": [
    "Here matrix ğ´=[[1,1,0],[0,1,0],[1,0,1]].  We compute the condition number and rank for this matrix and for ğ‘¦=[[1],[2],[1]], we get the augmented matrix [A, y]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb066d53-72aa-4230-8bc5-11798b3c9829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[1 1 0]\n",
      " [0 1 0]\n",
      " [1 0 1]]\n",
      "Condition number:\n",
      " 4.048917339522305\n",
      "Rank:\n",
      " 3\n",
      "Augmented matrix:\n",
      " [[1 1 0 1]\n",
      " [0 1 0 2]\n",
      " [1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import \\\n",
    "             cond, matrix_rank\n",
    "\n",
    "A = np.array([[1,1,0],\n",
    "              [0,1,0],\n",
    "              [1,0,1]])\n",
    "\n",
    "print('A:\\n', A)\n",
    "print('Condition number:\\n', cond(A))\n",
    "print('Rank:\\n', matrix_rank(A))\n",
    "y = np.array([[1], [2], [1]])\n",
    "A_y = np.concatenate((A, y), axis = 1)\n",
    "print('Augmented matrix:\\n', A_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f68cd1a-d177-4425-80ed-9285fec70ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d94a3a1-bc38-4379-84b8-e8240daeee41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [ 2. -2.]\n",
      "Eigenvectors: [[ 0.89442719 -0.89442719]\n",
      " [ 0.4472136   0.4472136 ]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e20530f9-babd-497b-a9bd-639bd4c14bed",
   "metadata": {},
   "source": [
    "## Linear Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232a9eb3-0fef-4069-b261-450e5a631426",
   "metadata": {},
   "source": [
    "For vectors ğ‘¥ and ğ‘¦, and scalars ğ‘ and ğ‘, it is sufficient to say that a function, ğ¹, is a linear transformation if\n",
    "\n",
    "ğ¹(ğ‘ğ‘¥+ğ‘ğ‘¦)=ğ‘ğ¹(ğ‘¥)+ğ‘ğ¹(ğ‘¦)\n",
    "\n",
    "It can be shown that multiplying an ğ‘šÃ—ğ‘› matrix, ğ´, and an ğ‘›Ã—1 vector, ğ‘£, of compatible size is a linear transformation of ğ‘£. Therefore from this point forward, a matrix will be synonymous with a linear transformation function."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2528db19-d489-4919-85fc-d2c120b2811c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97d23481-0bf1-4ff0-9338-73874458d3be",
   "metadata": {},
   "source": [
    "A **system of linear equations** is a set of linear equations that share the same variables.  They can therefore be converted to matrix form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eecf2f8-34f2-4e3d-ac77-f3d31fb68158",
   "metadata": {},
   "source": [
    "$/ 4x + 3y -5z = 2 $\n",
    "\n",
    "\n",
    "4ğ‘¥+3ğ‘¦âˆ’5ğ‘§ = 2\n",
    "2ğ‘¥âˆ’4ğ‘¦+5ğ‘§ = 5\n",
    "7ğ‘¥+8ğ‘¦ = 3\n",
    "ğ‘¥+2ğ‘§ = -1\n",
    "9+ğ‘¦âˆ’6ğ‘§ = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5ff02-dc1d-4588-b65f-0a8d1b22a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Consider a system of linear equations in matrix form, ğ´ğ‘¥=ğ‘¦, where ğ´ is an ğ‘šÃ—ğ‘› matrix. Recall that this means there are ğ‘š equations and ğ‘› unknowns in our system. A solution to a system of linear equations is an ğ‘¥ in â„ğ‘› that satisfies the matrix form equation. Depending on the values that populate ğ´ and ğ‘¦, there are three distinct solution possibilities for ğ‘¥. Either there is no solution for ğ‘¥, or there is one, unique solution for ğ‘¥, or there are an infinite number of solutions for ğ‘¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6069ca7d-31aa-4fe6-a332-0a7174a85946",
   "metadata": {},
   "source": [
    "Solving linear equations is easy in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a18023f4-1049-4244-975a-3e6c7df91ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.20833333 -2.58333333 -0.18333333]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[4, 3, -5], \n",
    "              [-2, -4, 5], \n",
    "              [8, 8, 0]])\n",
    "y = np.array([2, 5, -3])\n",
    "\n",
    "x = np.linalg.solve(A, y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a5e55-becf-43c4-9c4f-61942861d761",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bdd1d4-6d8e-4c6c-9343-585ad334af6c",
   "metadata": {},
   "source": [
    "* Linear algebra is the foundation of many engineering fields.\n",
    "* Vectors can be considered as points in â„ğ‘›; addition and multiplication are defined on them, although not necessarily the same as for scalars.\n",
    "* A set of vectors is linearly independent if none of the vectors can be written as a linear combination of the others.\n",
    "* Matrices are tables of numbers. They have several important properties including the determinant, rank, and inverse.\n",
    "* A system of linear equations can be represented by the matrix equation ğ´ğ‘¥=ğ‘¦.\n",
    "* The number of solutions to a system of linear equations is related to the rank(ğ´) and the rank([ğ´,ğ‘¦]). It can be zero, one, or infinity.\n",
    "* We can solve the equations using Gauss Elimination, Gauss-Jordan Elimination, LU decomposition and Gauss-Seidel method.\n",
    "* Methods exist to find matrix inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401863ce-eb65-427c-9736-23d93b4e9ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "124ecdf2-a67a-4915-acc8-e28453fc649f",
   "metadata": {},
   "source": [
    "They have many applications, to name a few, finding the natural frequencies and mode shapes in dynamics systems, solving differential equations (we will see in later chapters), reducing the dimensions using principal components analysis, getting the principal stresses in the mechanics, and so on. Even the famous Googleâ€™s search engine algorithm - PageRank, uses the eigenvalues and eigenvectors to assign scores to the pages and rank them in the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb5314-a2ed-4fee-bf1f-486ca27d0ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import eig\n",
    "\n",
    "a = np.array([[0, 4], \n",
    "              [1, 0]])\n",
    "w,v=eig(a)\n",
    "print('Eigenvalues:', w)\n",
    "print('Eigenvectors:', v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
